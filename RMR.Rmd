---
title: "RMR Prediction"
author: "Quirin Manz"
date: "`r Sys.Date()`"
output: html_document
---

# Load Data

## Load Enable Data

```{r, load-enable}
require(readxl)
require(tidytable)
conflicted::conflict_prefer_all('tidytable', quiet = TRUE)
data_folder <- "data"
# load enable data
enable_data <- read_xlsx(file.path(data_folder, "enable_Datensatz_RMR._Erwachsene 1.xlsx"), skip = 2, na=c('NA', 'N/A', ''))
# fix compound values
compounds <- c(
  "acetic acid a", "butyric acid a", "propionic acid a",
  "2-Methylbutyrate a", "hexanoic acid c", "Isobutyrate a",
  "Isovalerate a", "pentanoic acid a",
  "4-Methylvaleric acid a", "Lactic acid a"
)
# for all columns that have a comma in their name, replace commas in the column with a dot
for (column_name in names(enable_data)) {
  if (is.character(enable_data[[column_name]])) {
    enable_data[[column_name]] <-
      sub(",", ".", enable_data[[column_name]], fixed = TRUE)
    if (column_name %in% compounds) {
      # replace "<0" with 0
      enable_data[[column_name]] <-
        ifelse(enable_data[[column_name]] == "<0" |
                 enable_data[[column_name]] == "< 0", 0, enable_data[[column_name]])
    }
  }
}

data_csv <- file.path(data_folder, 'enable_data.tsv')
fwrite(enable_data, file = data_csv)
enable_data <- fread(data_csv, stringsAsFactors = TRUE, na=c('NA', 'N/A', ''))

enable_data[Site == "nuernber", `GEWICHt_SECA, kg` := `GEWICHt_SECA, kg` + 1]
# remove unnecessary column:
if(all(enable_data[, `Probanden-ID` == Label]))
  invisible(enable_data[, `Probanden-ID` := NULL])
```

## Load Microbiome Data

```{r, load-micro}
# load microbiome mapping
microbiome_mapping <- data.table::setDT(read_xlsx(file.path(data_folder, "mapping_file 1.xlsx")))
microbiome_mapping[, sample_clean := data.table::tstrsplit(".", x = `#SampleID`, fixed = TRUE, keep = 2)]
# check uniqueness
stopifnot(!any(microbiome_mapping[, duplicated(sample_clean)]))

# load microbiome data
microbiome_data <- fread(file.path(data_folder, "tax.summary.all 1.tab"))
stripped_names <- unlist(data.table::tstrsplit(".", x = names(microbiome_data), fixed = TRUE, keep = 2))
# check uniqueness
stopifnot(!any(duplicated(stripped_names)))
# clean names
names(microbiome_data) <- stripped_names
# get taxa names
taxa_names <- microbiome_data[, V1]
# transpose data
microbiome_data <- microbiome_data |> data.table::transpose(keep.names = "sample_clean", make.names = "V1")
# merge microbiome data with mapping
microbiome_data[microbiome_mapping, on=.(sample_clean), c("Label", "Cohort"):=.(Code, as.character(Cohort))]
# merge with enable data
invisible(enable_data[microbiome_data, on=.(Label), (c(taxa_names, "Cohort")) := mget(c(taxa_names, "Cohort"))])
# create field for first letter of label, i.e., categorical Age and Site
invisible(enable_data[, Label_group:=substr(Label, 1, 1)])

```

### Add Alpha Diversity

```{r, alpha-diversity}
# diversity columns
diversity_columns <- c("Shannon.effective", "Simpson.effective")
# load diversity data
diversity_data <- fread('data/Final table.tab', stringsAsFactors=TRUE) |> data.table::setnames(old='V1', new="Label") |> select(Label, all_of(diversity_columns))

invisible(enable_data[diversity_data, on=.(Label), (c(diversity_columns)) := mget(diversity_columns)])
```

## Set Variable Groups

```{r, load-column-groups}
# groups for taxa
taxa_group <-
  c(
    k = 'kingdom',
    p = 'phylum',
    c = 'class',
    o = 'order',
    f = 'family',
    g = 'genus'
  )[data.table::tstrsplit(taxa_names,
                          split = '__',
                          keep = 1,
                          fixed = TRUE)[[1]]]
# match the columns to certain groups
column_groups <- rbind(data.table::data.table(column = 'Label_group', group = "General information"),
                       fread(file.path(data_folder, 'column_groups.tsv')),
                       data.table::data.table(column = taxa_names, group = paste("microbiome", taxa_group)),
                       data.table::data.table(column = diversity_columns, group = paste("microbiome diversity")))
column_groups[, group:=as.factor(group)]

# set general response variable
response_variable <- "RMR.KJ"
# Conversion factor kcal to kilojoule
conversion_factor <- 4.184
# define grouping column
grouping_column <- "Label_group"
# get other labelling columns
other_labelling_columns <- c("Label", "Site", "Cohort", "Datum_V1")
# set the predictor of the established model
basic_predictors <- c("SEX", "Alter, Jahre", "FETTMASSE_SECA, kg", "FFM_SECA,kg")
# define microbial predictors (family level)
microbiome_predictors <- column_groups[group == 'microbiome family', column]
# # other microbial columns
# other_microbial_columns <- setdiff(taxa_names, microbial_predictors)
# define the general predictors
general_predictors <- setdiff(names(enable_data), c(response_variable, grouping_column, other_labelling_columns, taxa_names))
```

## Load Kiel Data

```{r, load-kiel}
kiel_data <- data.table::setDT(read_xlsx(file.path(data_folder, "REE Datenbank TUM_Januar 2025tf QM.xlsx")))
# make new SampleID from ID and zusätzliche Codierung
kiel_data[, Site := "kiel"]
kiel_data[, Label := paste0(ID, "_", `zusätzliche Codierung`)]
kiel_data[, SEX := factor(ifelse(`Geschlecht` == 1, "männlich", "weiblich"), levels = levels(enable_data$SEX))]
kiel_data[, `Alter, Jahre` := Alter]
kiel_data[, `GROESSE, cm` := `Größe (m)` * 100]
kiel_data[, `FETTMASSE_SECA, kg` := FM_ADP_kg]
kiel_data[, `GEWICHt_SECA, kg` := `Gewicht (kg)`]
kiel_data[, `FFM_SECA,kg` := `GEWICHt_SECA, kg` - `FETTMASSE_SECA, kg`]
kiel_data[, Datum_V1 := `Datum`]
kiel_data[, mean_temp := `daily mean T`]
kiel_data[, `RMR.KJ` := `REE (kcal)` * conversion_factor]

kiel_predictors <- setdiff(intersect(names(enable_data), names(kiel_data)), 
                             other_labelling_columns)
# remove response from munich_predictors
kiel_predictors <- setdiff(kiel_predictors, response_variable)

kiel_data[, (names(enable_data)[!names(enable_data) %in% names(kiel_data)]) := NA]
kiel_only <- setdiff(names(kiel_data), names(enable_data))
kiel_data[, (kiel_only) := NULL]
kiel_data <- kiel_data[, names(enable_data), with=FALSE]
```


## Load Munich Data
```{r, load-munich}
munich_data <- data.table::setDT(readxl::read_xlsx('data/RMR_munich.xlsx'))
munich_data_extension <- data.table::setDT(readxl::read_xlsx('data/Datenbank_GU_Daten an Frau Keppner_22.02.2019.xlsx'))
munich_data_extension2 <- data.table::setDT(readxl::read_xlsx('data/Datenbank_GU_Daten an Frau Keppner_22.02.2019_Ergänzung 31.07.23.xlsx'))
# munich_data[, `GEWICHt_SECA, kg` := fm + ffm]
munich_data[munich_data_extension, on=c(Label='ID'), c('GROESSE, cm', 'GEWICHt_SECA, kg', 'SEX') :=.(Größe, Gewicht, Geschl)]
munich_data[munich_data_extension2, on=c(Label='ID Datensatz Keppner'), Datum_V1 := `Dat. GU ORI-Datensatz`]
munich_data[, `FFM_SECA,kg` := ffm]
munich_data[, `FETTMASSE_SECA, kg` := fm]
munich_data[, `Körpertemperatur, °C` := body.temperature]
munich_data[, `Herzfrequenz, Schläge pro Minute` := as.integer(HF)]
munich_data[, `Blutdruck_systolisch, mmHg` := as.integer(systolic)]
munich_data[, `Alter, Jahre` := age]
# munich_data[, SEX := paste0(SEX, 'ich')]
munich_data[, SEX := ifelse(sex == 2, yes = 'weiblich', no = 'männlich')]
munich_data[, SEX := factor(SEX, levels = levels(enable_data$SEX))]
munich_data[, Site:=factor('munich')]

munich_predictors <- setdiff(intersect(names(enable_data), names(munich_data)), 
                             other_labelling_columns)
# remove response from munich_predictors
munich_predictors <- setdiff(munich_predictors, response_variable)

munich_data[, (names(enable_data)[!names(enable_data) %in% names(munich_data)]) := NA]
munich_only <- setdiff(names(munich_data), names(enable_data))
# remove all column in munich_only by reference from munich_data
munich_data[, (munich_only) := NULL]
# reorder column in munich_data to have the same order as enable_data
munich_data <- munich_data[, names(enable_data), with=FALSE]
```


```{r, merge-all}
all_data <- rbind(rbind(enable_data, munich_data, use.names=TRUE), kiel_data, use.names=TRUE)
all_data[, alter_group := cut(`Alter, Jahre`, breaks = c(min(`Alter, Jahre`, na.rm = TRUE) - 1, 30, 65, max(`Alter, Jahre`, na.rm = TRUE)))]
all_data[, Site:=factor(Site, levels = c('freising', 'munich', 'kiel', 'nuernber'))]
all_data[, table(SEX, alter_group, Site)]
# extract year from Datum_V1
all_data[, year := format(Datum_V1, '%Y')]
all_data[, month := format(Datum_V1, '%m')]
all_data[, half_year := cut(Datum_V1, breaks = seq(as.POSIXct("2006-03-01"), as.POSIXct("2019-03-01"), by = "6 month"))]
# extract month from half_year
all_data[, half_year:=data.table::tstrsplit(half_year, '-', fixed =TRUE, keep=2)]

#TODO
all_data[, season := cut(Datum_V1, breaks = seq(as.POSIXct("2006-08-06"), as.POSIXct("2019-08-06"), by = "3 month"))]
# extract month from half_year
all_data[, season:=data.table::tstrsplit(season, '-', fixed =TRUE, keep=2)]
```

```{r, load-sunlight}
process_sun_data <- function(sun_filename, sun_column = "SD_SO") {
  # Read the sun data and store its column names.
  sun_data <- data.table::fread(file.path(data_folder, sun_filename))
  sun_cols <- names(sun_data)
  
  # Create a DATE column from MESS_DATUM (formatted as "%Y%m%d%H").
  sun_data[, Datum_V1 := as.POSIXct(substr(as.character(MESS_DATUM), 1, 8), tz = "UTC", format = "%Y%m%d")]
  
  # Recode SD_SO to NA if equal to -999.
  sun_data[get(sun_column) == -999, (sun_column) := NA]

  return(sun_data)
}

# Define site-specific parameters.
sun_filenames <- c(munich="muc_sun_dwd.txt", freising="dürnast_sun_dwd.txt", nuernber="nürnberg_sun_dwd.txt", kiel="kiel_sun_dwd.txt")

# Use mapply to process each site and get a list of cleaned sun data tables.
result_list <- sapply(sun_filenames,
                      function(sun_file) {
  process_sun_data(sun_file)
}, simplify = FALSE)

# Combine the results into one data.table with an id column "Site" and set factor levels.
all_sun_data <- data.table::rbindlist(result_list, idcol = "Site")

# Print the combined table.
print(all_sun_data)

daily_sunlight <- all_sun_data[, .(minutes_sunlight = sum(SD_SO, na.rm = TRUE)), by=.(Site, Datum_V1)]
daily_sunlight[, Site := factor(Site, levels = all_data[, levels(Site)])]
all_data[daily_sunlight, on=c("Site", "Datum_V1"), minutes_sunlight := minutes_sunlight]

general_predictors <- c(general_predictors, "minutes_sunlight")
```

```{r, load-daylength}
daylengths <- data.table::setDT(read_xlsx(file.path(data_folder, "daylength_mod.xlsx"), sheet = "length all"))
stopifnot(daylengths[, .N == 365])
data.table::setnames(daylengths, old = c("münchen", "nürberg"), c('munich', 'nuernber'))
daylengths[, freising := munich]
daylengths[, day := .I]
melted_daylengths <- data.table::melt(daylengths, id.vars = "day", measure.vars = c('freising', 'munich', 'kiel', 'nuernber'), variable.name = "Site", value.name = "daylength")
melted_daylengths[, Site := factor(Site, levels=c('freising', 'munich', 'kiel', 'nuernber'))]
all_data[, day := as.integer(format(Datum_V1, format = "%j"))]
all_data[melted_daylengths, on=.(day, Site), daylength := daylength]
all_data[, day := NULL]
general_predictors <- c(general_predictors, "daylength")
```


## Check Distributions

```{r, plot-distributions}
require(ggpubr)
ggplot2::theme_set(ggplot2::theme_bw() + theme(strip.background = element_rect(fill = "white")))
# check distributions
all_data[!is.na(alter_group)] |> ggplot(aes(x=`Körpertemperatur, °C`, color=SEX)) + geom_density(alpha=0.5) + stat_summary(aes(xintercept = after_stat(x), y = 0), fun = median, geom = "vline", orientation = "y", linetype = 'dashed') + facet_grid(Site~alter_group) + scale_color_manual(values=c('männlich'='steelblue', 'weiblich'='tomato'))
all_data[!is.na(alter_group)] |> ggplot(aes(x=`Alter, Jahre`, color=SEX)) + geom_density(alpha=0.5) + stat_summary(aes(xintercept = after_stat(x), y = 0), fun = median, geom = "vline", orientation = "y", linetype = 'dashed') + facet_grid(Site~alter_group) + scale_color_manual(values=c('männlich'='steelblue', 'weiblich'='tomato'))
all_data[!is.na(alter_group)] |> ggplot(aes(x=`GEWICHt_SECA, kg`, color=SEX)) + geom_density(alpha=0.5) + stat_summary(aes(xintercept = after_stat(x), y = 0), fun = median, geom = "vline", orientation = "y", linetype = 'dashed') + facet_grid(Site~alter_group) + scale_color_manual(values=c('männlich'='steelblue', 'weiblich'='tomato'))
all_data[!is.na(alter_group)] |> ggplot(aes(x=`FFM_SECA,kg`, color=SEX)) + geom_density(alpha=0.5) + stat_summary(aes(xintercept = after_stat(x), y = 0), fun = median, geom = "vline", orientation = "y", linetype = 'dashed') + facet_grid(Site~alter_group) + scale_color_manual(values=c('männlich'='steelblue', 'weiblich'='tomato'))
all_data[!is.na(alter_group)] |> ggplot(aes(x=mean_temp, color=SEX)) + geom_density(alpha=0.5) + stat_summary(aes(xintercept = after_stat(x), y = 0), fun = median, geom = "vline", orientation = "y", linetype = 'dashed') + facet_grid(Site~alter_group) + scale_color_manual(values=c('männlich'='steelblue', 'weiblich'='tomato'))
all_data[!is.na(alter_group)] |> ggplot(aes(x=RMR.KJ, color=SEX)) + geom_density(alpha=0.5) + stat_summary(aes(xintercept = after_stat(x), y = 0), fun = median, geom = "vline", orientation = "y", linetype = 'dashed') + facet_grid(Site~alter_group) + scale_color_manual(values=c('männlich'='steelblue', 'weiblich'='tomato'))
```


```{r, fit-temp}
# create nls model
temp_by_date <- nls(mean_temp ~ A * sin(B*as.numeric(Datum_V1)/86400+C) + D, data = all_data, start=list(A=15, B=2 * pi / 365, C = 0, D=10))
# Create a sequence of dates from the minimum to maximum date
all_dates <- data.table(Datum_V1 = seq(
  from = all_data[, min(Datum_V1)],
  to = all_data[, max(Datum_V1)],
  by = "day"
))
# Predict temperatures for these dates using the nls model
all_dates[, predicted_temp := predict(temp_by_date, newdata = all_dates)]

all_data[!is.na(Datum_V1)] |> ggplot(aes(Datum_V1, y=mean_temp, color = Site)) + 
    geom_line(data = all_dates, color= 'grey50', aes(Datum_V1, predicted_temp)) + 
    geom_point(alpha=0.5)

all_data[!is.na(Datum_V1)] |> ggplot(aes(Datum_V1, y=mean_temp, color = Site)) + 
     geom_smooth(color = 'grey50', aes(color = NULL), se=FALSE, 
                 formula = y ~ A * sin(B*as.numeric(x)/86400+C) + D,
                 method='nls', method.args=list(start=list(A=10, B=2 * pi / 365, C = 0, D=10))) +
     geom_point(alpha=0.5)
```

```{r, plots-vs}
scatter <- ggscatterhist(all_data, x = "mean_temp", y = "RMR.KJ", color = "Site", alpha=.5)
scatter$sp <- scatter$sp + geom_smooth(method = 'lm', se = FALSE, aes(color = Site)) + stat_cor(size = 3, aes(color = Site))
print(scatter)

scatter <- ggscatterhist(all_data, x = "minutes_sunlight", y = "RMR.KJ", color = "Site", alpha=.5)
scatter$sp <- scatter$sp + geom_smooth(method = 'lm', se = FALSE, aes(color = Site)) + stat_cor(size = 3, aes(color = Site))
print(scatter)

scatter <- ggscatterhist(all_data, x = "daylength", y = "RMR.KJ", color = "Site", alpha=.5)
scatter$sp <- scatter$sp + geom_smooth(method = 'lm', se = FALSE, aes(color = Site)) + stat_cor(size = 3, aes(color = Site))
print(scatter)

scatter <- ggscatterhist(all_data, x = "daylength", y = "mean_temp", color = "Site", alpha=.5)
scatter$sp <- scatter$sp + geom_smooth(method = 'lm', se = FALSE, aes(color = Site)) + stat_cor(size = 3, aes(color = Site))
print(scatter)

ggplot(all_data[!is.na(alter_group)], aes(x = daylength, y = RMR.KJ, color = Site)) + geom_point(alpha = .5) + geom_smooth(method = 'lm', se = FALSE) + stat_cor(size = 3) + facet_grid(alter_group ~ SEX)

ggplot(all_data[!is.na(alter_group)], aes(y = RMR.KJ, x = mean_temp, color = SEX)) + 
  geom_point(alpha = .5) + geom_smooth(method = 'lm') + 
  facet_grid(alter_group ~ Site) + stat_cor(size = 3)

ggplot(all_data[!is.na(alter_group) & !is.na(RMR.KJ)], aes(y = RMR.KJ, x = mean_temp, color = alter_group)) + 
    geom_smooth(se=FALSE, method = 'lm') +
    geom_point(alpha = .5) +
    facet_grid(Site ~ year) + 
    stat_cor(data = all_data[, if(.N>2) .(RMR.KJ, mean_temp), by=.(year, Site, alter_group)], size = 3, na.rm = TRUE)

ggplot(all_data[!is.na(alter_group) & !is.na(Datum_V1)], aes(y = RMR.KJ, x = Datum_V1, color = SEX)) + 
  geom_smooth(se=FALSE, method = 'lm') +
  geom_point(alpha = .5) +
  facet_grid(alter_group ~ Site, scales = 'free_x') + 
  stat_cor(size = 3)

ggplot(all_data[!is.na(alter_group) & !is.na(Datum_V1)], aes(y = RMR.KJ, x = Datum_V1, color = SEX)) + 
  geom_smooth(se=FALSE) +
  geom_point(alpha = .5) +
  facet_grid(alter_group ~ Site, scales = 'free_x') + 
  stat_cor(size = 3)

ggplot(all_data[!is.na(alter_group) & !is.na(Datum_V1)], aes(y = RMR.KJ, x = Datum_V1, color = SEX)) + 
  geom_line(data = all_dates, color= 'grey50', aes(Datum_V1, -2000 * sin(0.0174*as.numeric(Datum_V1)/86400+4.1686) + 7750)) + 
  geom_point(alpha = .5) +
  facet_grid(alter_group ~ Site, scales = 'free_x') + 
  stat_cor(size = 3)

ggplot(all_data[!is.na(alter_group) & !is.na(Datum_V1)], aes(y = RMR.KJ, x = Datum_V1, color = SEX)) + 
  geom_smooth() +
  geom_point(alpha = .5) +
  facet_grid(year ~ Site, scales = 'free_x') + 
  stat_cor(size = 3)

ggplot(all_data[!is.na(alter_group) & !is.na(Datum_V1)], aes(y = mean_temp, x = Datum_V1, color = SEX)) + 
  geom_point(alpha = .5) + 
  geom_line(data = all_dates, color= 'grey50', aes(Datum_V1, predicted_temp)) + 
  facet_grid(alter_group ~ Site) + 
  stat_cor(size = 3)

ggplot(all_data[!is.na(alter_group)], aes(y = mean_temp, x = alter_group)) + geom_boxplot() + facet_wrap( ~ Site) + stat_compare_means(comparisons = combn(all_data[, levels(alter_group)], 2, simplify = FALSE)) + stat_summary(fun=mean, geom="point", shape=5)

ggplot(all_data, aes(x = mean_temp, y = `GEWICHt_SECA, kg`, color = SEX)) + geom_point() + facet_grid( ~ Site) + geom_smooth(method = 'lm') + stat_cor(size = 3)
ggplot(all_data[!is.na(Datum_V1)], aes(x = Datum_V1, y = `GEWICHt_SECA, kg`, color = SEX)) + geom_point() + facet_grid( ~ Site) + stat_cor(size = 3)


ggplot(all_data, aes(x = mean_temp, y = `FFM_SECA,kg`, color = SEX)) + geom_point() + facet_grid( ~ Site) + geom_smooth(method = 'lm') + stat_cor(size = 3)
ggplot(all_data[!is.na(Datum_V1)], aes(x = Datum_V1, y = `FFM_SECA,kg`, color = SEX)) + geom_point() + facet_grid( ~ Site) + stat_cor(size = 3)
```


```{r, load-weather}
process_weather_data <- function(filename, external_data, date_cols = NULL, 
                                 sheet = NULL, fix_temp = TRUE, 
                                 external_date_col = "Datum_V1", join_col = "RMR.KJ") {
  # Load required packages
  require(data.table)
  require(readxl)
  
  # Read the Excel file (sheet defaults to NULL)
  file_path <- file.path(data_folder, filename)
  # read excel if file ends with .xlsx else use data.table::fread
  if (endsWith(filename, ".xlsx")) {
    weather <- setDT(read_xlsx(file_path, sheet = sheet))
  } else {
    weather <- fread(file_path)
  }
  
  # Optionally fix temperature values (e.g. recode -999 to NA)
  if (fix_temp) {
    weather[TT_TU == -999, TT_TU := NA]
  }
  
  # Remove rows with missing values in the first five columns
  weather <- na.omit(weather, cols = 1:5)
  
  # Stop if there is any NA in the temperature column
  stopifnot(!any(is.na(weather$TT_TU)))
  
  # By default, use the first three columns for the date if not provided
  if (is.null(date_cols)) {
    date_cols <- names(weather)[1:3]
  }
  
  # Create a date column using the specified (or default) date columns
  weather[, date := as.POSIXct(paste(get(date_cols[1]), get(date_cols[2]), get(date_cols[3]), sep = "-"), 
                              tz = "UTC")]
  
  # Filter to rows with dates present in the external dataset and compute summary statistics
  weather_day <- weather[date %in% external_data[[external_date_col]],
                         .(min_temp   = min(TT_TU),
                           max_temp   = max(TT_TU),
                           mean_temp  = mean(TT_TU),
                           sd_temp    = sd(TT_TU),
                           range_temp = max(TT_TU) - min(TT_TU)),
                         by = date]
  
  # Merge the external data to add the join column (assumes external_data contains the specified columns)
  ext_subset <- external_data[, .(date_ext = get(external_date_col), 
                                  value   = get(join_col))]
  weather_day <- merge(weather_day, ext_subset, by.x = "date", by.y = "date_ext", all.x = TRUE)
  setnames(weather_day, "value", join_col)
  
  # Define summary variables to correlate with the join column
  summary_vars <- c("min_temp", "max_temp", "mean_temp", "sd_temp", "range_temp")
  
  # Compute correlations for each summary variable
  corr_pearson <- sapply(summary_vars, function(v) cor(weather_day[[v]], weather_day[[join_col]], method = "pearson", use = "pairwise.complete.obs"))
  corr_spearman <- sapply(summary_vars, function(v) cor(weather_day[[v]], weather_day[[join_col]], method = "spearman", use = "pairwise.complete.obs"))
  
  corr_table <- data.table(Variable = summary_vars, Pearson = corr_pearson, Spearman = corr_spearman)
  
  # Print the correlation table
  # setnames(corr_table, "Variable", paste(filename, sheet))
  print(corr_table)
  
  # Return only the aggregated weather table
  return(list(weather = weather_day, corrs = corr_table))
}

# muc_weather[, .(year=format(DATE, "%Y"), month=format(DATE, "%m"), day=format(DATE, "%d"), hour = format(DATE, "%H"), TT_TU)]
# muc_file2 <- "wetter_MUC_dwd.csv"
# fwrite(muc_weather[, .(year=format(DATE, "%Y"), month=format(DATE, "%m"), day=format(DATE, "%d"), hour = format(DATE, "%H"), TT_TU)], 
#        file = file.path(data_folder, muc_file2))

# Define site-specific parameters as vectors/lists
sites <- c("freising", "nuernber", "munich", "kiel")#, "munich2")
filenames <- c("Daten_mit_Wetter_Zeitraum.xlsx", "Daten_mit_Wetter_Zeitraum.xlsx", 
               "Daten_mit_Wetter_Zeitraum.xlsx", "wetter_KIEL_zeitraum.xlsx"
               #, muc_file2
               )
sheets <- list("Dürnast", "Nürnberg", "München_Stadt", NULL
               # , NULL
               )
external_data_list <- list(enable_data[Site == "freising"], enable_data[Site == "nuernber"], munich_data, kiel_data#, munich_data
                           )

# Use mapply to process each site and obtain a list of aggregated tables.
result_list <- mapply(function(file, sheet, ext_data) {
  process_weather_data(filename = file, external_data = ext_data,  sheet = sheet)
}, filenames, sheets, external_data_list, SIMPLIFY = FALSE)
names(result_list) <- sites

# Combine all the results into one big data.table with an id column "Site"
all_day_weather <- data.table::rbindlist(sapply(result_list, `[[`, 'weather', simplify = FALSE), idcol = "Site")
# Set factor levels for Site as desired
all_day_weather[, Site := factor(Site, levels = c("freising", "munich", "kiel", "nuernber"))] #, "munich2"))]

scatter <- ggscatterhist(all_day_weather, x = "mean_temp", y = "RMR.KJ", color = "Site", alpha=.5)
scatter$sp <- scatter$sp + geom_smooth(method = 'lm', se = FALSE, aes(color = Site)) + stat_cor(size = 3, aes(color = Site))
print(scatter)

scatter <- ggscatterhist(all_day_weather, x = "range_temp", y = "RMR.KJ", color = "Site", alpha=.5)
scatter$sp <- scatter$sp + geom_smooth(method = 'lm', se = FALSE, aes(color = Site)) + stat_cor(size = 3, aes(color = Site))
print(scatter)

# all_data[all_day_weather, on=c(Datum_V1='date', Site='Site'), 
#          c('mean_temp2', 'range_temp'):=.(mean_temp, range_temp)]
```

```{r, remove-plotting-artifacts}
all_data[, alter_group := NULL]
all_data[, Site:=factor(Site, levels = c('freising', 'nuernber', 'munich', 'kiel'))]
all_data[, year := NULL]
all_data[, half_year := NULL]
all_data[, month := NULL]
all_data[, season := NULL]
```


# Build Models

## Split and Check Data

```{r, prepare-models}
suppressPackageStartupMessages(library(tidymodels))
tidymodels_prefer()

# number of NA responses
message(paste("Number of NAs in response: ", all_data |> filter(is.na(`RMR.KJ`)) |> nrow()))
all_data <- all_data |> subset(!is.na(`RMR.KJ`))

# set seed for reproducibility
set.seed(1)
# split the data by Site to have a test sets
enable_split <- group_initial_split(all_data[Site %in% c('freising', 'nuernber')], group = "Site")
test_data <- enable_split |> testing()
train_data <- enable_split |> training()

# ensure that Site == "freising" in the whole train_data
stopifnot(all(train_data$Site == 'freising'))

# Number of samples that have at least one NA value
message(paste("Number of NAs in any variable: ", train_data |>
  filter_all(any_vars(is.na(.))) |>
  nrow()))

# # filter such that all samples have no NA values
# full_train_data <- train_data |>
#   filter_all(all_vars(!is.na(.)))

# set number of folds and also register parallel processing
nfolds <- 5
nrepeats <- 10

set.seed(1102)
repeated_cv_split <- rsample::vfold_cv(train_data, v = nfolds, repeats = nrepeats)
grouped_split <- rsample::group_vfold_cv(train_data, all_of(grouping_column), v = 3)

# munich splits:
# set seed for reproducibility
set.seed(1234)
# now we also need an initial split for the last_fit
munich_split <- group_initial_split(all_data[Site %in% c('munich', 'nuernber')], group = "Site")
# ensure that Site == "munich" in the whole train_data
stopifnot(all(munich_split |> training() |> pull(Site) == 'munich'))
# first the cv split
munich_repeated_cv_split <- rsample::vfold_cv(munich_split |> training(), v = nfolds, repeats = nrepeats)

set.seed(12)
# mixed kiel/enable splits:
kiel_split <- group_initial_split(all_data[Site %in% c('kiel', 'nuernber')], group = "Site")
stopifnot(all(kiel_split |> testing() |> pull(Site) == 'nuernber'))
kiel_repeated_cv_split <- rsample::vfold_cv(kiel_split |> training(), v = nfolds, repeats = nrepeats)
```

## Configure Models

```{r, configure-models}
# create recipes
general_recipe <-
  recipe(train_data) |>
  update_role(all_of(general_predictors), new_role = "predictor") |>
  update_role(all_of(response_variable), new_role = "outcome") |>
  update_role(all_of(other_labelling_columns), new_role = "labels") |>
  update_role(all_of(grouping_column), new_role = "splitting indicator") |>
  update_role(all_of(taxa_names), new_role = "microbiome") |>
  update_role(all_of(microbiome_predictors), new_role = "predictor") |>
  # step_mutate(`Alter, Jahre, orig` = `Alter, Jahre`) %>% # does not work because the name won't be available to remove later
  # step_cut(`Alter, Jahre`, breaks = c(30, 65), include_outside_range = TRUE) |>
  step_log(all_of(microbiome_predictors), offset = 1) |>
  step_impute_mean(all_numeric_predictors()) |>
  step_zv(all_predictors())

# general_predictors <- c(general_predictors, "Alter, Jahre, orig")

orig_scale_recipe <- general_recipe  |>
  step_dummy(all_nominal_predictors()) 

normalized_recipe <- general_recipe  |>
  step_normalize(all_numeric_predictors()) |> 
  step_dummy(all_nominal_predictors())

accessible_predictors <- kiel_predictors[kiel_predictors !="mean_temp"]

orig_scale_accessible_pred_recipe <- orig_scale_recipe |>
  update_role(all_of(general_predictors), new_role = "old_predictor") |>
  update_role(all_of(microbiome_predictors), new_role = "old_predictor") |>
  update_role(all_of(accessible_predictors), new_role = "predictor")

normalized_scale_accessible_pred_recipe <- normalized_recipe |>
  update_role(all_of(general_predictors), new_role = "old_predictor") |>
  update_role(all_of(microbiome_predictors), new_role = "old_predictor") |>
  update_role(all_of(accessible_predictors), new_role = "predictor")

kiel_recipe <-
  recipe(kiel_split |> training()) |>
  update_role(all_of(general_predictors), new_role = "old_predictor") |>
  update_role(all_of(response_variable), new_role = "outcome") |>
  update_role(all_of(other_labelling_columns), new_role = "labels") |>
  update_role(all_of(grouping_column), new_role = "splitting indicator") |>
  update_role(all_of(taxa_names), new_role = "microbiome") |>
  update_role(all_of(microbiome_predictors), new_role = "microbiome") |>
  update_role(all_of(kiel_predictors), new_role = "predictor") |>
  step_impute_mean(all_numeric_predictors()) |>
  step_zv(all_predictors())

# general_predictors <- c(general_predictors, "Alter, Jahre, orig")

kiel_orig_scale_recipe <- kiel_recipe  |>
  step_dummy(all_nominal_predictors()) 

kiel_normalized_recipe <- kiel_recipe  |>
  step_normalize(all_numeric_predictors()) |> 
  step_dummy(all_nominal_predictors())


munich_recipe <- 
  recipe(munich_split |> training()) |>
  update_role(all_of(general_predictors), new_role = "old_predictor") |>
  update_role(all_of(response_variable), new_role = "outcome") |>
  update_role(all_of(other_labelling_columns), new_role = "labels") |>
  update_role(all_of(grouping_column), new_role = "splitting indicator") |>
  update_role(all_of(taxa_names), new_role = "microbiome") |>
  update_role(all_of(microbiome_predictors), new_role = "microbiome") |>
  update_role(all_of(munich_predictors), new_role = "predictor") |>
  step_impute_mean(all_numeric_predictors()) |>
  step_zv(all_predictors())

munich_orig_scale_recipe <- munich_recipe  |>
  step_dummy(all_nominal_predictors()) 

munich_normalized_recipe <- munich_recipe  |>
  step_normalize(all_numeric_predictors()) |> 
  step_dummy(all_nominal_predictors())

# basic_recipe <- general_recipe |>
#   update_role(all_of(general_predictors), new_role = "old_predictor") |>
#   update_role(all_of(basic_predictors), new_role = "predictor")

# meanImpute_recipe <- general_recipe |>
#   update_role(all_of(microbiome_predictors), new_role = "predictor")

# interaction_recipe <- meanImpute_recipe |>
#   step_poly(all_numeric_predictors(), degree = 2) |>
#   step_interact(~ all_predictors():all_predictors())
  
# recipe list
recipe_list <- list(# basic = basic_recipe,
                    # general = general_recipe,
                    # interactions = interaction_recipe,
                    normalized = normalized_recipe,
                    origScale = orig_scale_recipe,
                    normalizedAccessPred = normalized_scale_accessible_pred_recipe,
                    origScaleAccessPred = orig_scale_accessible_pred_recipe,
                    munichNormalized = munich_normalized_recipe,
                    munichOrigScale = munich_orig_scale_recipe,
                    kielNormalized = kiel_normalized_recipe,
                    kielOrigScale = kiel_orig_scale_recipe)

main_metric <- 'rmse'
my_metrics <- metric_set(rmse, ccc, rsq, rsq_trad, mae)

# linear model
lm_model <- 
  linear_reg() |> 
  set_engine("lm")

# elastic net model
elnet_model <-
  linear_reg(penalty = tune(), mixture = tune()) |>
  set_engine("glmnet")

# lasso model
lasso_model <- 
  linear_reg(penalty = tune(), mixture = 1) |> 
  set_engine("glmnet", standardize = TRUE)

# random forest
rf_model <-
  rand_forest(trees = tune(), min_n = tune(), mtry = tune()) |> 
  set_engine("ranger", importance = 'permutation') |> 
  set_mode("regression")

# neural network
nnet_model <- 
   mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) |> 
   set_engine("nnet", MaxNWts = 2600) |> 
   set_mode("regression")

# model list
model_list <- list(linear = lm_model,
                   # elnet = elnet_model,
                   lasso = lasso_model,
                   RF = rf_model,
                   nnet = nnet_model)

# set the same parameters for all penalties
penalty_range <- penalty(range=c(-10, 3))

# set the parameters for the elnet
elnet_params <-
  elnet_model |>
  extract_parameter_set_dials() |>
  update(penalty = penalty_range)

# set the parameters for the lasso
lasso_params <-
  lasso_model |>
  extract_parameter_set_dials() |>
  update(penalty = penalty_range)

# set the parameters for the RF
rf_params <-
  rf_model |>
  extract_parameter_set_dials() 

# set the parameters for the NN
nnet_params <- 
   nnet_model |> 
   extract_parameter_set_dials() |> 
   update(hidden_units = hidden_units(c(1, 27))) |>
   update(penalty = penalty_range)

params_list <- list(elnet = elnet_params,
                   lasso = lasso_params,
                   RF = rf_params,
                   nnet = nnet_params)

```

## Configure Workflows

```{r, add-params}
grid_size <- 100
# set the workflows
workflows <- workflow_set(
      preproc = c(rep(recipe_list['origScale'], 3), 
                  rep(recipe_list['normalized'], 2),
                  rep(recipe_list['origScaleAccessPred'], 3), 
                  rep(recipe_list['normalizedAccessPred'], 2)), 
      models = rep(c(model_list,
                     model_list['lasso']), 2),
      cross = FALSE)

workflows <- workflows |>
    option_add(grid = grid_size)
 
# iterate over the wflow_ids, add paramters and generate the mtry parameter dynamically
for (this_wflow_id in workflows$wflow_id){
  model_name <- data.table::tstrsplit(this_wflow_id, '_', fixed = TRUE, keep = 2)[[1]]
  # set the parameters for the RF in question
  params <- params_list[[model_name]]
  if (model_name == "RF"){
    params <- params |> 
    update(mtry = mtry(c(
      1, sum(recipe_list[[sub("_.*", "", this_wflow_id)]]$var_info$role == "predictor")
    ))) 
  }
  workflows <- workflows |> 
    option_add(param_info = params, id = this_wflow_id)
}

kiel_workflows <- workflow_set(
      preproc = c(rep(recipe_list['kielOrigScale'], 3), 
                  rep(recipe_list['kielNormalized'], 2)), 
      models = c(model_list,
                 model_list['lasso']),
      cross = FALSE)

kiel_workflows <- kiel_workflows |>
    option_add(grid = grid_size)
 
# iterate over the wflow_ids, add paramters and generate the mtry parameter dynamically
for (this_wflow_id in kiel_workflows$wflow_id){
  model_name <- data.table::tstrsplit(this_wflow_id, '_', fixed = TRUE, keep = 2)[[1]]
  # set the parameters for the RF in question
  params <- params_list[[model_name]]
  if (model_name == "RF"){
    params <- params |> 
    update(mtry = mtry(c(
      1, sum(recipe_list[[sub("_.*", "", this_wflow_id)]]$var_info$role == "predictor")
    ))) 
  }
  kiel_workflows <- kiel_workflows |> 
    option_add(param_info = params, id = this_wflow_id)
}

munich_workflows <- workflow_set(
      preproc = c(rep(recipe_list['munichOrigScale'], 3), 
                  rep(recipe_list['munichNormalized'], 2)),
      models = c(model_list,
                 model_list['lasso']),
      cross = FALSE)

munich_workflows <- munich_workflows |> 
    option_add(grid = grid_size)

for (this_wflow_id in munich_workflows$wflow_id){
  model_name <- data.table::tstrsplit(this_wflow_id, '_', fixed = TRUE, keep = 2)[[1]]
  # set the parameters for the RF in question
  params <- params_list[[model_name]]
  if (model_name == "RF"){
    params <- params |> 
    update(mtry = mtry(c(
      1, sum(recipe_list[[sub("_.*", "", this_wflow_id)]]$var_info$role == "predictor")
    ))) }
  munich_workflows <- munich_workflows |> 
    option_add(param_info = params, id = this_wflow_id)
}

ctrl_parallel_over <- "everything"
# define the grid
grid_ctrl <-
   control_grid(
      save_pred = TRUE,
      save_workflow = TRUE,
      parallel_over = ctrl_parallel_over
   )

bayes_ctrl <-
    control_bayes(
      save_pred = TRUE,
      save_workflow = TRUE,
      parallel_over = ctrl_parallel_over
    )
```

## Train Models

```{r, train-models}
# train or load the models, depending on availibility
# if (! file.exists("cv_results.rds")) {
#   # train the models
require(doMC)
if (ctrl_parallel_over == 'everything') {
  doMC::registerDoMC(cores = nfolds * nrepeats)
} else {
  doMC::registerDoMC(cores = nrepeats)
}
  # All operating systems
  # require(doParallel)
  # 
  # # Create a cluster object and then register: 
  # cl <- makePSOCKcluster(nrepeats)
  # registerDoParallel(cl)
  
  # grid_cv_result <- ifelse(file.exists("grid_cv_results.rds"), readRDS("grid_cv_results.rds"), list)
split_list <- list(grouped = grouped_split, 
                   repeated = repeated_cv_split,
                   munich = munich_repeated_cv_split, 
                   kiel = kiel_repeated_cv_split)
cv_results <- mapply(names(split_list), 
                     list(workflows, 
                          workflows,
                          munich_workflows, 
                          kiel_workflows), 
                     FUN=function(this_split_name, this_workflows) {
  print(this_split_name)
  this_split <- split_list[[this_split_name]]
  grid_result_file <-
    file.path('fits', paste("grid", this_split_name, "results.rds", sep = "_"))
  # if (this_split_name == 'repeated') grid_result_file <- file.path('fits', paste("grid", this_split_name, "resultsMUCPRED.rds", sep = "_"))
  if (file.exists(grid_result_file)) {
    print("loading grid results")
    grid_results <- readRDS(grid_result_file)
  } else {
    print("creating grid results")
    grid_results <- this_workflows |>
      workflow_map(
        fn = "tune_grid",
        seed = 1503,
        resamples = this_split,
        control = grid_ctrl,
        verbose = TRUE,
        metrics = my_metrics
      )
    saveRDS(grid_results, grid_result_file)
  }
  return(grid_results)
  # we only do grid search
  bayes_result_file <-
    file.path('fits', paste("bayes", this_split_name, "results.rds", sep = "_"))
  if (file.exists(bayes_result_file)) {
    print("loading bayes results")
    bayes_results <- readRDS(bayes_result_file)
  } else {
    print("creating bayes results")
    wf_with_initial <- this_workflows |>
      option_remove(grid)
    for (this_wflow_id in this_workflows$wflow_id) {
      wf_with_initial <- wf_with_initial |>
        option_add(
          id = this_wflow_id,
          initial = grid_results |> extract_workflow_set_result(id = this_wflow_id)
        )
    }
    bayes_results <- wf_with_initial |>
      workflow_map(
        fn = "tune_bayes",
        seed = 1503,
        resamples = this_split,
        iter = 50,
        control = bayes_ctrl,
        verbose = TRUE,
        metrics = my_metrics
      )
    saveRDS(bayes_results, bayes_result_file)
  }
  return(bayes_results)
}, SIMPLIFY = FALSE)
  # stopCluster(cl)
  
names(cv_results) <- names(split_list)
#   saveRDS(cv_results, "cv_results.rds")
# } else {
#   # load the models
#   cv_results <- readRDS("cv_results.rds")
# }
```

## Evaluate Models and Select Best

```{r, plot-results-select-best, fig.width=10, fig.height=6}
best_models_per_split <- sapply(names(cv_results), function(this_split_name){
  print(this_split_name)
  this_result <- cv_results[[this_split_name]]
  print(autoplot(this_result, type="wflow_id") + labs(title = this_split_name))
  
  wflow_ids <- this_result$wflow_id
  wflow_ids_wo_linear <- wflow_ids[!endsWith(wflow_ids, "_linear")]
  
  if (this_split_name == 'kiel') {
    this_workflows <- kiel_workflows
  } else if (this_split_name == 'munich') {
    this_workflows <- munich_workflows
  } else {
    this_workflows <- workflows
  }

  final_models <- sapply(wflow_ids_wo_linear, function(this_wflow_id) {
    wflow_results <- this_result |> 
      extract_workflow_set_result(this_wflow_id)
    print(autoplot(wflow_results) + labs(title=paste(this_split_name, this_wflow_id)))
    # print(autoplot(wflow_results, type = 'performance') + labs(title = paste(this_split_name, this_wflow_id)))
    specs <- this_workflows |> extract_spec_parsnip(this_wflow_id)
    tuning_params <- specs |> extract_parameter_set_dials() |> pull(name)
    if (inherits(specs, "linear_reg")) {
      if (all(c("penalty", "mixture") %in% tuning_params))
        best_model <- wflow_results |> select_by_one_std_err(metric = main_metric, desc(penalty), mixture)
      else if ("penalty" %in% tuning_params)
        best_model <- wflow_results |> select_by_one_std_err(metric = main_metric, desc(penalty))
      else if ("mixture" %in% tuning_params)
        best_model <- wflow_results |> select_by_one_std_err(metric = main_metric, mixture)
    } else if (inherits(specs, "rand_forest")) {
      best_model <- wflow_results |> select_by_one_std_err(metric = main_metric, trees, mtry, desc(min_n))
    } else if (inherits(specs, "mlp")) {
      best_model <- wflow_results |> select_by_one_std_err(metric = main_metric, epochs, hidden_units, desc(penalty))
    }
    
    # list(one_std_err_results = best_model, 
    #      fit = this_result |>
    #        extract_workflow(this_wflow_id) |>
    #        finalize_workflow(best_model) |>
    #        last_fit(split = enable_split, metrics=my_metrics))
    best_model
    
  }, simplify = FALSE)

  one_std_err_results <-
    data.table::rbindlist(
      final_models,
      idcol = 'wflow_id',
      fill = TRUE
    ) |>
    select(wflow_id, .config)
  best_results <- this_result |> 
    rank_results(rank_metric = main_metric, select_best = TRUE) |>
    select(wflow_id, .config) |>
    unique()
  results_to_keep <- rbind(one_std_err_results, best_results)
  
  cv_result_dt <- this_result |> rank_results(rank_metric = main_metric) |> semi_join(results_to_keep)
  cv_result_dt[, rank := factor(rank, levels = sort(unique(rank)))]
  
  print(ggplot(cv_result_dt, aes(x = rank, y = mean, color = wflow_id)) +
    geom_point() +
    geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = .5) +
    facet_wrap(~.metric, scales = 'free') + labs(title = this_split_name))
  
  if (this_split_name == 'kiel') {
    this_split <- kiel_split
  } else if (this_split_name == 'munich') {
    this_split <- munich_split
  } else {
    this_split <- enable_split
  }

  final_fits <- sapply(names(final_models), function(this_wflow_id)
    this_result |>
           extract_workflow(this_wflow_id) |>
           finalize_workflow(final_models[[this_wflow_id]]) |>
           last_fit(split = this_split, metrics=my_metrics),
    simplify = FALSE)
  
  linear_models <- sapply(wflow_ids[!wflow_ids %in% wflow_ids_wo_linear], function(this_wflow_id){
    this_recipe_name <- sub(pattern = '_.*$', replacement = '', this_wflow_id)
    workflow() |> 
      add_recipe(recipe_list[[this_recipe_name]]) |>
      add_model(lm_model) |> 
      last_fit(split = this_split, metrics = my_metrics)
  }, simplify = FALSE)
  list(fits=c(final_fits, linear_models), cv_results = cv_result_dt |> semi_join(one_std_err_results))
}, simplify = FALSE)
cv_result_dt <- data.table::rbindlist(sapply(best_models_per_split, function(x) x$cv_results, simplify = FALSE), idcol = 'split')
best_models_per_split <- sapply(best_models_per_split, function(x) x$fits, simplify = FALSE)
```


### Evaluate LASSO Feature Robustness

```{r, lasso-robustness, fig.width=8, fig.height=8}
library(glmnet)

feature_colors <- c(
  "FFM_SECA,kg" = "#009999",          # Teal (greenish for FFM_SECA)
  "GEWICHt_SECA, kg" = "#B2DF8A",    # Light Green (for GEWICHt_SECA)
  "daylength" = "darkgrey",           # Dark Gray (for daylength)
  "mean_temp" = "#000000",           # Black (for mean_temp)
  "GFRCKDE, mg/min" = "#ff6db6",     # Pink (blood measure GFRCKDE)
  "FT3, pg/ml" = "#b66dff",          # Purple (blood measure FT3)
  "MCHC, g/dl" = "#490092",          # Dark Purple (blood measure MCHC)
  "unexplained" = "#db6d00",         # Orange (for unexplained)
  "individual variation" = "#FDB462" # Light Orange (for individual variation)
)

stability_dt <- data.table::rbindlist(mapply(c('origScale', 'origScaleAccessPred', 'kielOrigScale', 'munichOrigScale'), 
                                             list(repeated_cv_split, repeated_cv_split, kiel_repeated_cv_split, munich_repeated_cv_split), 
                                             FUN = function(this_recipe_name, my_split){
  this_recipe <- recipe_list[[this_recipe_name]]
  # prepare x and y 
  prepped_training_data <- this_recipe |> step_select(c(all_predictors(), all_outcomes())) |> prep() |> bake(NULL)
  x <- prepped_training_data |> select(-any_of(response_variable)) |> as.matrix()
  y <- prepped_training_data |> pull(response_variable)
  
  # get the repeats from our repeated split
  repeat_ids <- my_split$id |> unique()
  
  # lasso runs per repeat
  cvfits <- 
    lapply(repeat_ids, function(repeat_id) {
    # Filter the splits for the specified repeat
    repeat_splits <- my_split |> subset(id == repeat_id)
    
    # Initialize a fold ID vector (length = number of rows in the data)
    fold_ids <- integer(prepped_training_data |> nrow())
    
    # Assign fold numbers to each observation based on assessment sets
    for (i in seq_along(repeat_splits$splits)) {
      fold <- repeat_splits$splits[[i]]
      assessment_indices <- tidy(fold) |> subset(Data == "Assessment") |> pull(Row)
      fold_ids[assessment_indices] <- i  # Assign fold number `i` to assessment indices
    }
    stopifnot(all(fold_ids>0))
    
    lasso <-
      cv.glmnet(
        x = x,
        y = y,
        alpha = 1,
        family = "gaussian",
        foldid = fold_ids,
        standardize = TRUE
      )
    
    lasso
  }) 
  names(cvfits) <- repeat_ids
  
  nonzero_coefs <- lapply(cvfits, function(cvfit) {
    # get the nonzero coefficients
    coefs <- coef(cvfit, s = "lambda.1se")
    nonzero_coefs <- coefs[coefs[, 1] != 0, ]
    sort(nonzero_coefs, decreasing = TRUE)
    names(nonzero_coefs)
    })
  names(nonzero_coefs) <- repeat_ids
  
  transposed_df <- stack(nonzero_coefs)
  # remove intercept from values
  transposed_df <- data.table::setDT(transposed_df) |> subset(values != "(Intercept)")
  transposed_df
}, SIMPLIFY = FALSE), idcol = 'recipe')

ggplot(stability_dt, aes(x = reorder(values, ind, function(x) -length(x)), fill = values)) +
    geom_bar() +
    scale_fill_manual(values = feature_colors) +
    facet_grid(recipe ~.) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = 'none') +
    labs(title = "LASSO Feature Robustness", x = "Feature", y = "Count")

ggplot(stability_dt[recipe == 'origScale'], aes(x = reorder(values, ind, function(x) -length(x)), fill = values)) +
    geom_bar() +
    scale_fill_manual(values = feature_colors) +
    # facet_grid(recipe ~.) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = 'none') +
    labs(title = "LASSO Feature Robustness", x = "Feature", y = "Count")
```

### Evaluate Best Lasso Coefficients

```{r, tidy-lasso-coefs}
# LASSO can be used for feature selection only, so now we will build a more complex model with the selected variables from LASSO
normalized_lasso_coefs <- best_models_per_split$repeated$normalized_lasso |> extract_fit_parsnip() |> tidy() |> subset(estimate != 0) |> mutate(abs_estimate = abs(estimate)) |> arrange(desc(abs_estimate)) |> select(-abs_estimate)
print(normalized_lasso_coefs)

normalized_muc_pred_lasso_coefs <- best_models_per_split$repeated$normalizedAccessPred_lasso |> extract_fit_parsnip() |> tidy() |> subset(estimate != 0) |> mutate(abs_estimate = abs(estimate)) |> arrange(desc(abs_estimate)) |> select(-abs_estimate)
print(normalized_muc_pred_lasso_coefs)

normalized_lasso_coefs <- data.table::rbindlist(
    list(`all predictors` = normalized_lasso_coefs, `muc predictors` = normalized_muc_pred_lasso_coefs),
    idcol = 'preds'
  ) 
normalized_lasso_coefs <- normalized_lasso_coefs |> subset(term != '(Intercept)')
normalized_lasso_coefs[, term:=factor(term, levels=normalized_lasso_coefs[order(abs(estimate), decreasing = TRUE), unique(term)])]

# Function to calculate luminance
calculate_luminance <- function(hex_color) {
  rgb <- col2rgb(hex_color) / 255
  # Apply relative luminance formula
  luminance <- 0.2126 * rgb[1, ] + 0.7152 * rgb[2, ] + 0.0722 * rgb[3, ]
  return(luminance)
}

# Decide text color (black or white) based on luminance threshold
text_colors <- sapply(feature_colors, function(color) {
  if (calculate_luminance(color) > 0.5) {
    "black"  # Light background -> Black text
  } else {
    "white"  # Dark background -> White text
  }
})

ggplot(normalized_lasso_coefs[preds == 'all predictors'],
       aes(x = estimate,
           y = reorder(term, abs(estimate)),
           fill = term)) +
    geom_col() +
    scale_fill_manual(values = feature_colors) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + #, legend.position = 'none') +
    theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) +
    # facet_grid(preds ~ .) +
    labs(x = "LASSO Coefficient on Standardized Features", y = NULL, fill = NULL)

ggplot(normalized_lasso_coefs,
       aes(y = estimate,
           x = reorder(term, -abs(estimate)),
           fill = preds)) +
    geom_col(position = 'dodge') +
    # scale_fill_manual(values = feature_colors) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + #, legend.position = 'none') +
    # theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) + 
    # facet_grid(preds ~ .) +
    labs(x = "LASSO Coefficient on Standardized Features")#, x = NULL, fill = NULL)
```


```{r, tidy-lasso-coefs-origScale}
print("grouped coefs:")
best_models_per_split$grouped$origScale_lasso |> extract_fit_parsnip() |> tidy() |> subset(estimate != 0) |> mutate(abs_estimate = abs(estimate)) |> arrange(desc(abs_estimate)) |> select(-abs_estimate)

selected_coefs <- best_models_per_split$repeated$origScale_lasso |> extract_fit_parsnip() |> tidy() |> subset(estimate != 0) |> mutate(abs_estimate = abs(estimate)) |> arrange(desc(abs_estimate)) |> select(-abs_estimate)
print("selected coefs:")
print(selected_coefs)
print("selected muc pred coefs:")
print(best_models_per_split$repeated$origScaleAccessPred_lasso |> extract_fit_parsnip() |> tidy() |> subset(estimate != 0) |> mutate(abs_estimate = abs(estimate)) |> arrange(desc(abs_estimate)) |> select(-abs_estimate))
print("selected kiel coefs:")
print(best_models_per_split$kiel$kielOrigScale_lasso |> extract_fit_parsnip() |> tidy() |> subset(estimate != 0) |> mutate(abs_estimate = abs(estimate)) |> arrange(desc(abs_estimate)) |> select(-abs_estimate))
```


### Build More Complex Models

```{r, complex-models, fig.width=10, fig.height=6}
selected_features <- selected_coefs |> subset(term != '(Intercept)') |> pull(term)

# now let's make another recipe using only those features
selected_recipe <- orig_scale_recipe |>
  update_role(all_of(general_predictors), new_role = "old_predictor") |>
  update_role(all_of(microbiome_predictors), new_role = "old_predictor") |>
  update_role(all_of(selected_features), new_role = "predictor")

quadratic_recipe <- selected_recipe |>
  step_poly(all_numeric_predictors(), degree = 2) 

interaction_recipe <- selected_recipe |>
  step_interact(~ all_predictors():all_predictors())

woblood_recipe <- selected_recipe |>
  update_role(all_of(c("FT3, pg/ml", "GFRCKDE, mg/min", "MCHC, g/dl")), new_role = "old_predictor")

wotemp_recipe <- selected_recipe |>
  update_role(all_of(c("FT3, pg/ml", "GFRCKDE, mg/min", "MCHC, g/dl", "mean_temp")), new_role = "old_predictor")

wodaylength_recipe <- selected_recipe |>
  update_role(all_of(c("FT3, pg/ml", "GFRCKDE, mg/min", "MCHC, g/dl", "daylength")), new_role = "old_predictor")

woboth_recipe <- selected_recipe |>
  update_role(all_of(c("FT3, pg/ml", "GFRCKDE, mg/min", "MCHC, g/dl", "mean_temp", "daylength")), new_role = "old_predictor")

bea_recipes <- orig_scale_recipe |>
  update_role(all_of(general_predictors), new_role = "old_predictor") |>
  update_role(all_of(microbiome_predictors), new_role = "old_predictor")

bea1_recipe <- bea_recipes |>
  update_role(all_of(c("SEX", "Alter, Jahre", "FETTMASSE_SECA, kg", "FFM_SECA,kg")), new_role = "predictor")

bea2_recipe <- bea_recipes |>
  update_role(all_of(c("Alter, Jahre", "FETTMASSE_SECA, kg", "FFM_SECA,kg", "Körpertemperatur, °C")), new_role = "predictor")

bea3_recipe <- bea_recipes |>
  update_role(all_of(c("Alter, Jahre", "FETTMASSE_SECA, kg", "FFM_SECA,kg", "Körpertemperatur, °C", "FT3, pg/ml", "LEUKOZYTEN/nl", "KREATIN, mg/dl", "MCHC, g/dl", "Blutdruck_systolisch, mmHg", "Herzfrequenz, Schläge pro Minute")), new_role = "predictor") 

bea4_recipe <- bea_recipes |>
  update_role(all_of(c("Alter, Jahre", "FETTMASSE_SECA, kg", "FFM_SECA,kg", "Körpertemperatur, °C", "FT3, pg/ml", "LEUKOZYTEN/nl", "KREATIN, mg/dl", "MCHC, g/dl", "Blutdruck_systolisch, mmHg", "Herzfrequenz, Schläge pro Minute",  "mean_temp")), new_role = "predictor")

new_recipes <- list(selected = selected_recipe, 
                    woblood = woblood_recipe,
                    wotemp = wotemp_recipe,
                    wodaylength = wodaylength_recipe,
                    woboth = woboth_recipe,
                    quadratic = quadratic_recipe, 
                    interaction = interaction_recipe,
                    bea1 = bea1_recipe,
                    bea2 = bea2_recipe,
                    bea3 = bea3_recipe,
                    bea4 = bea4_recipe)

new_recipe_cv_results <- workflow_set(
      preproc = new_recipes,
      models = list(linear = lm_model)) |>
      workflow_map(
        fn = "fit_resamples",
        seed = 1503,
        resamples = repeated_cv_split,
        control = grid_ctrl,
        verbose = TRUE,
        metrics = my_metrics
      ) |> rank_results() |> mutate(split = 'repeated', .before = 1)

ggplot(new_recipe_cv_results, aes(x = rank, y = mean, color = wflow_id)) +
    geom_point() +
    geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = .5) +
    facet_wrap(~.metric, scales = 'free')

cv_result_dt <- rbind(cv_result_dt, new_recipe_cv_results)
# rerank the results based on the main metric
cv_result_dt[, wflow_id2 := factor(wflow_id, levels = cv_result_dt[split == 'repeated' & .metric == main_metric][order(mean), wflow_id])]
ggplot(cv_result_dt[split == 'repeated'], aes(x = wflow_id2, y = mean, color = model)) +
    geom_point() +
    geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = .5) +
    facet_wrap(~.metric, scales = 'free') +
  # rotate x-axis labels 90 degree
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

recipe_list <- c(recipe_list, new_recipes)

new_models <- sapply(new_recipes, 
       function(this_recipe){
         workflow() |> 
           add_recipe(this_recipe) |>
           add_model(lm_model) |>
           last_fit(split = enable_split, metrics = my_metrics)
}, simplify = FALSE)
names(new_models) <- paste(names(new_models), 'linear', sep = "_")
best_models_per_split$repeated <- c(best_models_per_split$repeated, new_models)
```

#### Evaluate Models with Selected Features

```{r, selected-models, fig.width=11, fig.height=7}
require(ggrepel)
selected_lm_fits <- sapply(new_models,
       function(model) {
         lm_fit <- model |> extract_fit_engine()
         coef_mat <- lm_fit |> summary() |> coefficients()
         # coef_mat[coef_mat[, 'Pr(>|t|)'] < 0.05, , drop = FALSE] |> print()
         lm_fit
       }, simplify = FALSE)

feature_imps <- c("lmg", "pmvd")
linear_imp <- relaimpo::calc.relimp(selected_lm_fits$selected_linear, type = feature_imps)
expl_variance <- data.table::setDT(enframe(c(linear_imp$pmvd, `individual variation` = 0.043, unexplained = 0), 'Feature', 'Explained Variance'))
expl_variance[Feature == 'unexplained', `Explained Variance` := 1 - sum(expl_variance$`Explained Variance`)]
expl_variance[, Feature:=factor(Feature, levels=c(names(sort(linear_imp$pmvd, decreasing = TRUE)), 'unexplained', 'individual variation'))]
data.table::setkey(expl_variance, Feature)
expl_variance[, cum_var := cumsum(`Explained Variance`)]
expl_variance[, x_space := cum_var - `Explained Variance`/2]

ggplot(expl_variance, aes(x = `Explained Variance`, y = 'Feature', fill = reorder(Feature, -x_space))) + 
    geom_col(position = 'fill') + 
    scale_fill_manual(values = feature_colors, guide = guide_legend(reverse = TRUE, nrow = 1)) + 
    theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(), legend.position = 'top') + 
    ggrepel::geom_label_repel(aes(label = round(`Explained Variance` * 100, 1),
                                  x = x_space),
                              max.overlaps = 10,
                              min.segment.length = 0,
                              force = 10,
                              direction = 'y',
                              show.legend = FALSE,
                              color = text_colors[expl_variance[, levels(Feature)]],
                              segment.color = text_colors[expl_variance[, levels(Feature)]]) + 
    labs(y = NULL, fill = NULL)

ggplot(
  enframe((
    best_models_per_split$repeated$origScale_RF |> extract_fit_engine()
  )$variable.importance,
  'feature',
  'var_imp'
  ) |> arrange(desc(var_imp)) |> slice_head(n = 20),
  aes(x = var_imp, y = reorder(feature, var_imp))
) + geom_col()
```


### Current Methods in Literature

```{r, current-methods}
established_models <- c(`Harris-Benedict` = 'Harris-Benedict', `Kleiber` = 'Kleiber', WHO = 'WHO')



HarrisBenedict <- function(weight, height, age, sex_female){
  ifelse(sex_female, 
         655.1 + 9.6*weight + 1.8*height - 4.7*age,
         66.47 + 13.7*weight + 5*height - 6.8*age) * conversion_factor
}

Kleiber <- function(weight){
  283 * weight^0.75
}

WHO <- function(weight, age, sex_female) {
  
  # Initialize RMR vector
  RMR <- numeric(length(weight))
  
  # Define reusable age filters
  age_filter <- list(
    age_3_10 = age <= 18,
    age_10_30 = age >= 18 & age <= 30,
    age_30_60 = age > 30 & age <= 60,
    age_60_plus = age > 60
  )
  
  # Calculate RMR for each age group using `ifelse` conditioned on `sex_female`
  RMR[age_filter$age_3_10] <- ifelse(sex_female[age_filter$age_3_10],
                                     (22.5 * weight[age_filter$age_3_10] + 499),
                                     (22.7 * weight[age_filter$age_3_10] + 495))
  
  RMR[age_filter$age_10_30] <- ifelse(sex_female[age_filter$age_10_30],
                                      (14.7 * weight[age_filter$age_10_30] + 496),
                                      (15.3 * weight[age_filter$age_10_30] + 679))
  
  RMR[age_filter$age_30_60] <- ifelse(sex_female[age_filter$age_30_60],
                                      (8.7 * weight[age_filter$age_30_60] + 829),
                                      (11.6 * weight[age_filter$age_30_60] + 879))
  
  RMR[age_filter$age_60_plus] <- ifelse(sex_female[age_filter$age_60_plus],
                                        (10.5 * weight[age_filter$age_60_plus] + 596),
                                        (13.5 * weight[age_filter$age_60_plus] + 487))
  
  return(RMR*conversion_factor)
}

```

## Compute Training and Testing Errors

```{r, interesting-models}
interesting_models <- c(established_models, 
                          `LASSO Enable` = "origScale_lasso", 
                          LinearWoBlood = "woblood_linear", 
                          LinearWoBloodTemp = "wotemp_linear",
                          LinearWoBloodDaylength = "wodaylength_linear",
                          LinearWoBloodDaylengthTemp = "woboth_linear",
                          bea1 = "bea1_linear",
                          bea2 = "bea2_linear",
                          bea3 = "bea3_linear",
                          bea4 = "bea4_linear",
                          `LASSO Access` = "origScaleAccessPred_lasso", 
                          `LASSO Muc` = "munichOrigScale_lasso", 
                          `LASSO Kiel` = "kielOrigScale_lasso")

# Swap names and values
interesting_modelmap <- setNames(names(interesting_models), interesting_models)

cv_result_dt[wflow_id %in% interesting_models, wflow_id := interesting_modelmap[wflow_id]]


ggplot(cv_result_dt[split == 'repeated' & wflow_id %in% interesting_modelmap], aes(x = wflow_id, y = mean, color = model)) +
    geom_point() +
    geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = .5) +
    facet_wrap(~.metric, scales = 'free') +
  # rotate x-axis labels 90 degree
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# get the fits for the interesting models
interesting_fits <- c(best_models_per_split$munich[interesting_models], best_models_per_split$repeated[interesting_models], best_models_per_split$kiel[interesting_models])
# remove NULLs
interesting_fits <- interesting_fits[!sapply(interesting_fits, is.null)]

interesting_model_estimates <- data.table::rbindlist(lapply(interesting_fits, function(fit) fit |> extract_fit_parsnip() |> tidy() |> subset(estimate != 0) |> mutate(abs_estimate = abs(estimate)) |> arrange(desc(abs_estimate)) |> select(-abs_estimate)), idcol = 'model', fill = TRUE)

interesting_model_estimates[, model := interesting_modelmap[model]]

print(interesting_model_estimates[term != '(Intercept)', paste(.SD[order(-abs(estimate)), term], collapse = "; "), by = model])

ggplot(interesting_model_estimates[term != '(Intercept)'],
       aes(y = estimate,
           x = reorder(term, -abs(estimate)),
       fill = model)) +
    geom_col(position = 'dodge', color = 'white') + 
    # scale_fill_manual(values = feature_colors) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + #, legend.position = 'none') +
    # theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) + 
    # facet_grid(model ~ .) +
    labs(x = "Coefficients")#, x = NULL, fill = NULL)
```


```{r, helper-functions}
add_trained_on <- function(dt, model_column = 'model'){
  dt[, training:= "Freising"]
  dt[startsWith(get(model_column), prefix = 'munich'), training := 'Munich']
  dt[startsWith(get(model_column), prefix = 'kiel'), training := 'Kiel']
  # dt[, training := ifelse(startsWith(get(model_column), prefix = 'munich'), 'Munich', 'Freising')]
  dt[get(model_column) %in% established_models, training := 'Established']
  dt
}

predict_data_fits <- function(fits, recipe_list, truth, new_data = NULL) {
  data.table::rbindlist(
    sapply(names(fits), function(this_wflow_id) {
      last_fit <- fits[[this_wflow_id]]
      this_recipe_name <- sub(pattern = '_.*$', replacement = '', this_wflow_id)
      this_recipe <- recipe_list[[this_recipe_name]]
      prepped_data <- this_recipe |> step_select(all_of(c(all_predictors(), all_outcomes()))) |> prep() |> bake(new_data = new_data)
      last_fit |> extract_fit_parsnip() |> predict(new_data = prepped_data) |> mutate(truth = truth)
    }, simplify = FALSE),
    idcol = 'model'
  ) |> rename(estimate = .pred)
}

predict_data_established <- function(new_data, truth){
  data.table::rbindlist(list(
    data.table::data.table(model = established_models['Harris-Benedict'], 
                           estimate = HarrisBenedict(weight = new_data$`GEWICHt_SECA, kg`,
                                                     height = new_data$`GROESSE, cm`,
                                                     age = new_data$`Alter, Jahre`,
                                                     new_data$SEX == 'weiblich'),
                           truth = truth),
    data.table::data.table(model = established_models['Kleiber'], 
                           estimate = Kleiber(weight = new_data$`GEWICHt_SECA, kg`),
                           truth = truth),
    data.table::data.table(model = established_models['WHO'], 
                           estimate = WHO(weight = new_data$`GEWICHt_SECA, kg`, 
                                          age = new_data$`Alter, Jahre`,
                                          sex_female = new_data$SEX == 'weiblich'),
                           truth = truth)
    )
  )
}

compute_my_metrics <- function(preds, my_metrics, remove_from_string = c('ccc', 'mae')){
  metric_dt <- preds[, my_metrics(.SD, truth = truth, estimate = estimate), by=model]
  metric_dt <- data.table::dcast(metric_dt, model ~ .metric, value.var = '.estimate')
  metric_names <- names(metric_dt)[names(metric_dt) != 'model']
  metric_names <- metric_names[!metric_names %in% remove_from_string]
  metric_dt[, metrics_string := do.call(
      paste, 
      c(Map(function(name, value) paste(name, round(value, 2), sep = ": "), metric_names, mget(metric_names)), sep = "\n")
  )]
  add_trained_on(metric_dt)
}

plot_preds <- function(preds, metric_dt, main_metric, focus_observed = FALSE, n_row = 3){
  p <- ggplot(preds, aes(x = truth, y = estimate)) +
    geom_abline(color = "gray50", lty = 2) + 
    geom_point(alpha = 0.5) + 
    facet_wrap(~factor(model, levels = metric_dt[order(get(main_metric)), model]) + training, nrow = n_row) +
    labs(title = 'Errors', x = 'Observed', y = 'Predicted') +
    geom_label(data = metric_dt, aes(x = -Inf, y = Inf, label = metrics_string), 
               vjust = 1.1, hjust = -0.1, inherit.aes = FALSE, size = 3)
  if (focus_observed) {
    p <- p +
      coord_obs_pred(xlim=c(min(preds$truth), max(preds$truth)),
                     ylim=c(min(preds$truth), max(preds$truth)))
  } else {
    p <- p +
      coord_obs_pred()
  }
  p
}

plot_bland_altman <- function(preds, metric_dt, main_metric, n_row = 3) {
  preds[, mean_measurement:=(truth + estimate)/2]
  preds[, difference:= truth - estimate]
  preds[, mean_difference:=mean(difference), by=model]
  preds[, lower_difference:= mean_difference - sd(difference)*1.96, by=model]
  preds[, upper_difference:= mean_difference + sd(difference)*1.96, by=model]
  
  ggplot(preds, aes(x = mean_measurement, y = difference)) +
    geom_hline(aes(yintercept = mean_difference)) +
    geom_hline(aes(yintercept = lower_difference), linetype = 'dashed') +
    geom_hline(aes(yintercept = upper_difference), linetype = 'dashed') +
    geom_point(alpha = 0.5) + 
    facet_wrap(~factor(model, levels = metric_dt[order(get(main_metric)), model]) + training, nrow = n_row) +
    labs(title = 'Bland-Altman', x = 'Mean Measurement', y = 'Difference')
}


```


### Freising/Training Errors

```{r, training-errors, fig.width=12, fig.height=8}
freising_preds <- predict_data_fits(fits = best_models_per_split$repeated, 
                                    recipe_list = recipe_list, 
                                    truth = train_data$RMR.KJ)
freising_muc_preds <- predict_data_fits(fits = best_models_per_split$munich, 
                                    recipe_list = recipe_list,
                                    truth = train_data$RMR.KJ,
                                    new_data = train_data)
freising_kiel_preds <- predict_data_fits(fits = best_models_per_split$kiel, 
                                    recipe_list = recipe_list,
                                    truth = train_data$RMR.KJ,
                                    new_data = train_data)

freising_preds <- rbind(
  freising_preds,
  freising_muc_preds,
  freising_kiel_preds,
  predict_data_established(train_data, train_data$RMR.KJ)
)
add_trained_on(freising_preds)

freising_metrics <- compute_my_metrics(freising_preds, my_metrics)

plot_preds(freising_preds, freising_metrics, main_metric = main_metric)

plot_bland_altman(freising_preds, freising_metrics, main_metric)
```
```{r, freising-training-subset, fig.width=8, fig.height=6}
plot_preds(freising_preds[model %in% interesting_models], freising_metrics[model %in% interesting_models], main_metric = main_metric, n_row = 2)
```


### Nürnberg/Testing Errors
```{r, nuremberg-testing-errors, fig.width=12, fig.height=8}
nuremberg_preds <-
  data.table::rbindlist(
    sapply(
      c(best_models_per_split$repeated, best_models_per_split$munich, best_models_per_split$kiel),
      collect_predictions,
      simplify = FALSE
    ),
    idcol = 'model'
  ) |> rename(truth = RMR.KJ, estimate = .pred) |> select(model, estimate, truth) 

nuremberg_preds <- rbind(
  nuremberg_preds,
  predict_data_established(test_data, test_data$RMR.KJ)
)
add_trained_on(nuremberg_preds)

nuremberg_metrics <- compute_my_metrics(nuremberg_preds, my_metrics)

plot_preds(nuremberg_preds, nuremberg_metrics, main_metric = main_metric)

plot_preds(nuremberg_preds, nuremberg_metrics, main_metric = main_metric, focus_observed = TRUE)
plot_bland_altman(nuremberg_preds, nuremberg_metrics, main_metric)
```


```{r, nuremberg-testing-subset, fig.width=8, fig.height=6}
plot_preds(nuremberg_preds[model %in% interesting_models], nuremberg_metrics[model %in% interesting_models], main_metric = main_metric, n_row = 2)
```


```{r, nuremberg-testing-ba, fig.width=12, fig.height=4}
nuremberg_ba <- plot_bland_altman(nuremberg_preds[model %in% interesting_models], nuremberg_metrics, main_metric)
nuremberg_ba[["facet"]][["params"]][["nrow"]] <- 1
nuremberg_ba
```


### München/Testing Errors
```{r, munich-testing-errors, fig.width=12, fig.height=8}
munich_preds <- predict_data_fits(fits = best_models_per_split$repeated,
                                  recipe_list = recipe_list, 
                                  truth = munich_split |> training() |> pull(response_variable),
                                  new_data = munich_split |> training())

munich_munich_preds <- predict_data_fits(fits = best_models_per_split$munich,
                                  recipe_list = recipe_list, 
                                  truth = munich_split |> training() |> pull(response_variable))


munich_kiel_preds <- predict_data_fits(fits = best_models_per_split$kiel,
                                  recipe_list = recipe_list, 
                                  truth = munich_split |> training() |> pull(response_variable),
                                  new_data = munich_split |> training())

munich_preds <- rbind(
  munich_preds,
  munich_munich_preds,
  munich_kiel_preds,
  predict_data_established(munich_split |> training(), munich_split |> training() |> pull(response_variable))
)
add_trained_on(munich_preds)

munich_metrics <- compute_my_metrics(munich_preds, my_metrics)

plot_preds(munich_preds, munich_metrics, main_metric = main_metric)

plot_preds(munich_preds, munich_metrics, main_metric = main_metric, focus_observed = TRUE)
plot_bland_altman(munich_preds, munich_metrics, main_metric)
```


```{r, munich-testing-subset, fig.width=8, fig.height=6}
plot_preds(munich_preds[model %in% interesting_models], munich_metrics[model %in% interesting_models], main_metric = main_metric, n_row = 2)
```


```{r, munich-testing-ba, fig.width=12, fig.height=4}
munich_ba <- plot_bland_altman(munich_preds[model %in% interesting_models], munich_metrics[model %in% interesting_models], main_metric)
munich_ba[["facet"]][["params"]][["nrow"]] <- 1
munich_ba
```



### Kiel/Testing Errors
```{r, kiel-testing-errors, fig.width=12, fig.height=8}
kiel_preds <- predict_data_fits(fits = best_models_per_split$repeated,
                                  recipe_list = recipe_list, 
                                  truth = all_data[Site == 'kiel'] |> pull(response_variable),
                                  new_data = all_data[Site == 'kiel'])

kiel_munich_preds <- predict_data_fits(fits = best_models_per_split$munich,
                                  recipe_list = recipe_list, 
                                  truth = all_data[Site == 'kiel'] |> pull(response_variable),
                                  new_data = all_data[Site == 'kiel'])

kiel_kiel_preds <- predict_data_fits(fits = best_models_per_split$kiel,
                                  recipe_list = recipe_list, 
                                  truth = all_data[Site == 'kiel'] |> pull(response_variable))

kiel_preds <- rbind(
  kiel_preds,
  kiel_munich_preds,
  kiel_kiel_preds,
  predict_data_established(munich_split |> training(), munich_split |> training() |> pull(response_variable))
)
add_trained_on(kiel_preds)

kiel_metrics <- compute_my_metrics(kiel_preds, my_metrics)

plot_preds(kiel_preds, kiel_metrics, main_metric = main_metric)

plot_preds(kiel_preds, kiel_metrics, main_metric = main_metric, focus_observed = TRUE)
plot_bland_altman(kiel_preds, kiel_metrics, main_metric)
```


```{r, kiel-testing-subset, fig.width=8, fig.height=6}
plot_preds(kiel_preds[model %in% interesting_models], kiel_metrics[model %in% interesting_models], main_metric = main_metric, n_row = 2)
```


```{r, kiel-testing-ba, fig.width=12, fig.height=4}
kiel_ba <- plot_bland_altman(kiel_preds[model %in% interesting_models], kiel_metrics[model %in% interesting_models], main_metric)
kiel_ba[["facet"]][["params"]][["nrow"]] <- 1
kiel_ba
```

### Kiel/Testing Errors Without Temperature
```{r, kielWoTemp-testing-errors, fig.width=12, fig.height=8}
kielWoTemp_data <- all_data[Site == 'kiel']
kielWoTemp_data <- kielWoTemp_data |> mutate(mean_temp = NA)
kielWoTemp_preds <- predict_data_fits(fits = best_models_per_split$repeated,
                                  recipe_list = recipe_list, 
                                  truth = kielWoTemp_data |> pull(response_variable),
                                  new_data = kielWoTemp_data)

kielWoTemp_munich_preds <- predict_data_fits(fits = best_models_per_split$munich,
                                  recipe_list = recipe_list, 
                                  truth = kielWoTemp_data |> pull(response_variable),
                                  new_data = kielWoTemp_data)

kielWoTemp_kiel_preds <- predict_data_fits(fits = best_models_per_split$kiel,
                                  recipe_list = recipe_list, 
                                  truth = kielWoTemp_data |> pull(response_variable),
                                  new_data = kielWoTemp_data)

kielWoTemp_preds <- rbind(
  kielWoTemp_preds,
  kielWoTemp_munich_preds,
  kielWoTemp_kiel_preds,
  predict_data_established(munich_split |> training(), munich_split |> training() |> pull(response_variable))
)
add_trained_on(kielWoTemp_preds)

kielWoTemp_metrics <- compute_my_metrics(kielWoTemp_preds, my_metrics)

plot_preds(kielWoTemp_preds, kielWoTemp_metrics, main_metric = main_metric)

plot_preds(kielWoTemp_preds, kielWoTemp_metrics, main_metric = main_metric, focus_observed = TRUE)
plot_bland_altman(kielWoTemp_preds, kielWoTemp_metrics, main_metric)
```


```{r, kielWoTemp-testing-subset, fig.width=8, fig.height=6}
plot_preds(kielWoTemp_preds[model %in% interesting_models], kielWoTemp_metrics[model %in% interesting_models], main_metric = main_metric, n_row = 2)
```


```{r, kielWoTemp-testing-ba, fig.width=12, fig.height=4}
kielWoTemp_ba <- plot_bland_altman(kielWoTemp_preds[model %in% interesting_models], kielWoTemp_metrics[model %in% interesting_models], main_metric)
kielWoTemp_ba[["facet"]][["params"]][["nrow"]] <- 1
kielWoTemp_ba
```

### Kiel/Testing Errors Without Daylength
```{r, kielWoDaylength-testing-errors, fig.width=12, fig.height=8}
kielWoDaylength_data <- all_data[Site == 'kiel']
kielWoDaylength_data <- kielWoDaylength_data |> mutate(daylength = NA)
kielWoDaylength_preds <- predict_data_fits(fits = best_models_per_split$repeated,
                                  recipe_list = recipe_list, 
                                  truth = kielWoDaylength_data |> pull(response_variable),
                                  new_data = kielWoDaylength_data)

kielWoDaylength_munich_preds <- predict_data_fits(fits = best_models_per_split$munich,
                                  recipe_list = recipe_list, 
                                  truth = kielWoDaylength_data |> pull(response_variable),
                                  new_data = kielWoDaylength_data)

kielWoDaylength_kiel_preds <- predict_data_fits(fits = best_models_per_split$kiel,
                                  recipe_list = recipe_list, 
                                  truth = kielWoDaylength_data |> pull(response_variable),
                                  new_data = kielWoDaylength_data)

kielWoDaylength_preds <- rbind(
  kielWoDaylength_preds,
  kielWoDaylength_munich_preds,
  kielWoDaylength_kiel_preds,
  predict_data_established(munich_split |> training(), munich_split |> training() |> pull(response_variable))
)
add_trained_on(kielWoDaylength_preds)

kielWoDaylength_metrics <- compute_my_metrics(kielWoDaylength_preds, my_metrics)

plot_preds(kielWoDaylength_preds, kielWoDaylength_metrics, main_metric = main_metric)

plot_preds(kielWoDaylength_preds, kielWoDaylength_metrics, main_metric = main_metric, focus_observed = TRUE)
plot_bland_altman(kielWoDaylength_preds, kielWoDaylength_metrics, main_metric)
```


```{r, kielWoDaylength-testing-subset, fig.width=8, fig.height=6}
plot_preds(kielWoDaylength_preds[model %in% interesting_models], kielWoDaylength_metrics[model %in% interesting_models], main_metric = main_metric, n_row = 2)
```


```{r, kielWoDaylength-testing-ba, fig.width=12, fig.height=4}
kielWoDaylength_ba <- plot_bland_altman(kielWoDaylength_preds[model %in% interesting_models], kielWoDaylength_metrics[model %in% interesting_models], main_metric)
kielWoDaylength_ba[["facet"]][["params"]][["nrow"]] <- 1
kielWoDaylength_ba
```

### Kiel/Testing Errors Without Both
```{r, kielWoBoth-testing-errors, fig.width=12, fig.height=8}
kielWoBoth_data <- all_data[Site == 'kiel']
kielWoBoth_data <- kielWoBoth_data |> mutate(mean_temp = NA, daylength = NA)
kielWoBoth_preds <- predict_data_fits(fits = best_models_per_split$repeated,
                                  recipe_list = recipe_list, 
                                  truth = kielWoBoth_data |> pull(response_variable),
                                  new_data = kielWoBoth_data)

kielWoBoth_munich_preds <- predict_data_fits(fits = best_models_per_split$munich,
                                  recipe_list = recipe_list, 
                                  truth = kielWoBoth_data |> pull(response_variable),
                                  new_data = kielWoBoth_data)

kielWoBoth_kiel_preds <- predict_data_fits(fits = best_models_per_split$kiel,
                                  recipe_list = recipe_list, 
                                  truth = kielWoBoth_data |> pull(response_variable),
                                  new_data = kielWoBoth_data)

kielWoBoth_preds <- rbind(
  kielWoBoth_preds,
  kielWoBoth_munich_preds,
  kielWoBoth_kiel_preds,
  predict_data_established(munich_split |> training(), munich_split |> training() |> pull(response_variable))
)
add_trained_on(kielWoBoth_preds)

kielWoBoth_metrics <- compute_my_metrics(kielWoBoth_preds, my_metrics)

plot_preds(kielWoBoth_preds, kielWoBoth_metrics, main_metric = main_metric)

plot_preds(kielWoBoth_preds, kielWoBoth_metrics, main_metric = main_metric, focus_observed = TRUE)
plot_bland_altman(kielWoBoth_preds, kielWoBoth_metrics, main_metric)
```


```{r, kielWoBoth-testing-subset, fig.width=8, fig.height=6}
plot_preds(kielWoBoth_preds[model %in% interesting_models], kielWoBoth_metrics[model %in% interesting_models], main_metric = main_metric, n_row = 2)
```


```{r, kielWoBoth-testing-ba, fig.width=12, fig.height=4}
kielWoBoth_ba <- plot_bland_altman(kielWoBoth_preds[model %in% interesting_models], kielWoBoth_metrics[model %in% interesting_models], main_metric)
kielWoBoth_ba[["facet"]][["params"]][["nrow"]] <- 1
kielWoBoth_ba
```

### Kiel Temperature Comparison
```{r, kiel-temp-comparison, eval=FALSE}
kiel_temp_metrics <- data.table::rbindlist(list(mean = kielWoTemp_metrics, normal = kiel_metrics), idcol = 'temp') |>
  data.table::melt(id.vars = c('model', 'training', 'metrics_string', 'temp'), variable.name = 'metric_name', value.name = 'metric') |>
  data.table::dcast(model + training + metric_name ~ temp, value.var = 'metric')
ggplot(kiel_temp_metrics[model %in% interesting_models[!interesting_models %in% c('Kleiber', 'Harris-Benedict')] & metric_name %in% c('ccc', 'rsq', 'rsq_trad')], aes(x = normal, y = mean, color = model, shape = training)) +
  geom_point(position = 'dodge') +
  facet_wrap(~ metric_name) +
  geom_abline(color = "gray50", lty = 2) + 
  coord_obs_pred() +
  theme(legend.position = 'bottom')
ggplot(kiel_temp_metrics[model %in% interesting_models[!interesting_models %in% c('Kleiber', 'Harris-Benedict')] & metric_name %in% c('mae', 'rmse')], aes(x = normal, y = mean, color = model, shape = training)) +
  geom_point(position = 'dodge') +
  facet_wrap(~ metric_name) +
  geom_abline(color = "gray50", lty = 2) + 
  coord_obs_pred() +
  theme(legend.position = 'bottom')
```


### Summary
```{r, summary, fig.width=12, fig.height=8}
require(tidytext)
require(patchwork)
summary_metrics <- data.table::rbindlist(list(
  Freising = freising_metrics,
  Nuremberg = nuremberg_metrics,
  Munich = munich_metrics,
  Kiel = kiel_metrics,
  KielWoTemp = kielWoTemp_metrics,
  KielWoDaylength = kielWoDaylength_metrics,
  KielWoBoth = kielWoBoth_metrics
), idcol='testing') |> 
  data.table::melt(id.vars = c('model', 'testing', 'training', 'metrics_string'), variable.name = 'metric_name', value.name = 'metric')
# summary_metrics[, model := factor(model, levels = summary_metrics[testing == 'Nuremberg' & metric_name == 'rmse'][order(metric), model])]
interesting_metrics <- c('rmse', 'rsq_trad')
summary_metrics[startsWith(as.character(metric_name), 'rsq'), metric := metric * 100]
# lapply(interesting_metrics, function(this_metric) {
#   p <- summary_metrics[model %in% interesting_models] |> 
#     # subset(metric_name == this_metric) |> 
#     ggplot(aes(
#       x = tidytext::reorder_within(model, rmse, testing),
#       y = get(this_metric),
#       fill = training
#     )) +
#     geom_col(color = 'white') +
#     facet_wrap(~ testing, scales = "free") +
#     labs(x = 'Model', y = "Metric") + 
#     tidytext::scale_x_reordered() +
#     theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5))
#   if (this_metric == 'rmse')
#     p <- p + 
#           # coord_cartesian(ylim = c(0, 1000)) + 
#           theme(axis.text.x = element_blank())
#   p
# }) |> patchwork::wrap_plots(ncol = 1) + patchwork::plot_layout(guides = 'collect', axis = 'collect')
summary_metrics[, testing := factor(testing, levels = c('Nuremberg', 'Freising', 'Munich', 'Kiel', 'KielWoTemp', 'KielWoDaylength', 'KielWoBoth'))]
summary_metrics[, training := factor(training, levels = c('Freising', 'Munich', 'Kiel', 'Established'))]
summary_metrics[model %in% interesting_models, model := interesting_modelmap[model]]
summary_metrics[model %in% interesting_modelmap & !model %in% established_models & metric_name == "rsq_trad",]
summary_metrics[model %in% interesting_modelmap] |> 
    subset(metric_name %in% interesting_metrics) |> 
    ggplot(aes(
      x = tidytext::reorder_within(model, metric, testing),
      y = metric,
      fill = training
    )) +
    geom_col(color = 'white', alpha = 0.7) +
    # add the value as text into the bar, ninety degrees rotated
    geom_text(aes(label = round(metric, 2), y = 0), hjust = -.1, angle = 90) +
    facet_grid(metric_name ~ testing, scales = "free") +
    labs(x = 'Model', y = "Metric") + 
    tidytext::scale_x_reordered() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

subset_summaries <- summary_metrics[model %in% interesting_modelmap & 
                         !model %in% established_models &
                         metric_name == "rsq_trad"]
subset_summaries[, model:= factor(model, levels = rev(c("LASSO Enable", "LinearWoBlood", "LinearWoBloodTemp", "LinearWoBloodDaylength", "LinearWoBloodDaylengthTemp", paste0("bea", 1:4), "LASSO Access", "LASSO Kiel", "LASSO Muc")))]
ggplot(subset_summaries, aes(x = testing, y = model, fill = metric, label = round(metric, 2))) + 
    geom_tile() + geom_text() + 
    scale_fill_gradient(low = "white", high = "red", limits=c(0, 100)) + 
    labs(fill = "R squared")
```